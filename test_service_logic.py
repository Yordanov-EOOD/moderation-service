#!/usr/bin/env python3
"""
Service Logic Test Script
=========================

This script replicates the exact logic used in our moderation service
to isolate whether the issue is in the service integration or model logic.
"""

import os
import sys
import json
from datetime import datetime

# Add the service source path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

try:
    from services.toxicity_detector import ToxicityDetector
except ImportError as e:
    print(f"‚ùå Could not import ToxicityDetector: {e}")
    print("Make sure you're running from the moderation-service directory")
    sys.exit(1)

class ServiceLogicTester:
    """Test the exact service logic"""
    
    def __init__(self):
        print("üîß Initializing Service Logic Tester...")
        
        # Test messages with different severity levels
        self.test_messages = [
            # Clean content
            {"content": "I love this beautiful day!", "expected_toxic": False, "severity": "‚úÖ Clean"},
            {"content": "Thank you for your helpful feedback.", "expected_toxic": False, "severity": "‚úÖ Clean"},
            {"content": "This is a great article about technology.", "expected_toxic": False, "severity": "‚úÖ Clean"},
            {"content": "What a wonderful community we have here.", "expected_toxic": False, "severity": "‚úÖ Clean"},
            {"content": "The weather forecast shows rain tomorrow.", "expected_toxic": False, "severity": "‚úÖ Clean"},
            
            # Borderline content
            {"content": "I disagree with this opinion.", "expected_toxic": False, "severity": "üòê Neutral"},
            {"content": "This is quite frustrating to deal with.", "expected_toxic": False, "severity": "üòê Neutral"},
            {"content": "You're being annoying.", "expected_toxic": True, "severity": "‚ö†Ô∏è Mild"},
            {"content": "That's a stupid idea.", "expected_toxic": True, "severity": "‚ö†Ô∏è Mild"},
            {"content": "This is dumb.", "expected_toxic": True, "severity": "‚ö†Ô∏è Mild"},
            
            # Clearly toxic content
            {"content": "You're a complete idiot.", "expected_toxic": True, "severity": "üö® Toxic"},
            {"content": "Go kill yourself.", "expected_toxic": True, "severity": "üö® Toxic"},
            {"content": "I hate you so much.", "expected_toxic": True, "severity": "üö® Toxic"},
            {"content": "You're worthless and should die.", "expected_toxic": True, "severity": "üö® Toxic"},
            {"content": "F**k you, you piece of sh*t.", "expected_toxic": True, "severity": "üö® Toxic"},
            {"content": "Nobody likes you, loser.", "expected_toxic": True, "severity": "üö® Toxic"},
        ]
        
        # Initialize the detector (same as service)
        self.detector = ToxicityDetector(use_ml_model=True)
        
    def test_service_logic(self):
        """Test the service logic directly"""
        print("üß™ Testing Service Logic...")
        print("=" * 80)
        
        results = []
        correct_predictions = 0
        
        for i, test_case in enumerate(self.test_messages, 1):
            content = test_case["content"]
            expected = test_case["expected_toxic"]
            severity = test_case["severity"]
            
            print(f"\nüìù Test {i}/{len(self.test_messages)}: {severity}")
            print(f"üí¨ Content: \"{content}\"")
            print(f"üéØ Expected: {'TOXIC' if expected else 'CLEAN'}")
            
            try:
                # Call the exact same method our service uses
                result = self.detector.check_toxicity(content)
                
                # Extract key information
                is_toxic = result.get('is_toxic', False)
                toxicity_score = result.get('toxicity_score', 0.0)
                confidence = result.get('confidence', 0.0)
                categories = result.get('categories', [])
                detector_version = result.get('detector_version', 'unknown')
                model_used = result.get('model_used', 'unknown')
                threshold_used = result.get('threshold_used', 0.5)
                
                # Check if prediction is correct
                is_correct = (is_toxic == expected)
                if is_correct:
                    correct_predictions += 1
                    status_icon = "‚úÖ"
                else:
                    status_icon = "‚ùå"
                
                print(f"üìä Result: {'TOXIC' if is_toxic else 'CLEAN'} {status_icon}")
                print(f"üìà Toxicity Score: {toxicity_score:.4f}")
                print(f"üé≤ Confidence: {confidence:.4f}")
                print(f"üîß Threshold: {threshold_used}")
                print(f"üè∑Ô∏è  Categories: {categories}")
                print(f"ü§ñ Model: {model_used}")
                print(f"üì¶ Detector: {detector_version}")
                
                # Store result
                results.append({
                    **test_case,
                    "actual_toxic": is_toxic,
                    "toxicity_score": toxicity_score,
                    "confidence": confidence,
                    "categories": categories,
                    "is_correct": is_correct,
                    "full_result": result
                })
                
            except Exception as e:
                print(f"‚ùå Error: {e}")
                results.append({
                    **test_case,
                    "error": str(e),
                    "is_correct": False
                })
        
        # Calculate accuracy
        accuracy = correct_predictions / len(self.test_messages)
        
        print("\n" + "=" * 80)
        print("üìà SERVICE LOGIC TEST SUMMARY")
        print("=" * 80)
        print(f"‚úÖ Correct predictions: {correct_predictions}/{len(self.test_messages)}")
        print(f"üìä Accuracy: {accuracy:.2%}")
        
        # Breakdown by severity
        print("\nüìã Results by Severity:")
        severity_stats = {}
        for result in results:
            sev = result['severity']
            if sev not in severity_stats:
                severity_stats[sev] = {"correct": 0, "total": 0}
            severity_stats[sev]["total"] += 1
            if result.get('is_correct', False):
                severity_stats[sev]["correct"] += 1
        
        for sev, stats in severity_stats.items():
            sev_acc = stats["correct"] / stats["total"] if stats["total"] > 0 else 0
            print(f"  {sev}: {stats['correct']}/{stats['total']} ({sev_acc:.1%})")
        
        # Detailed analysis
        print("\nüî¨ DETAILED ANALYSIS:")
        
        # Check false positives (clean content marked as toxic)
        false_positives = [r for r in results if not r["expected_toxic"] and r.get("actual_toxic", False)]
        if false_positives:
            print(f"\n‚ùå False Positives ({len(false_positives)}):")
            for fp in false_positives:
                print(f"   ‚Ä¢ \"{fp['content']}\" - Score: {fp.get('toxicity_score', 'N/A')}")
        
        # Check false negatives (toxic content marked as clean)
        false_negatives = [r for r in results if r["expected_toxic"] and not r.get("actual_toxic", True)]
        if false_negatives:
            print(f"\n‚ùå False Negatives ({len(false_negatives)}):")
            for fn in false_negatives:
                print(f"   ‚Ä¢ \"{fn['content']}\" - Score: {fn.get('toxicity_score', 'N/A')}")
        
        # Score distribution analysis
        scores = [r.get('toxicity_score', 0) for r in results if 'toxicity_score' in r]
        if scores:
            print(f"\nüìä Score Distribution:")
            print(f"   ‚Ä¢ Min: {min(scores):.4f}")
            print(f"   ‚Ä¢ Max: {max(scores):.4f}")
            print(f"   ‚Ä¢ Average: {sum(scores)/len(scores):.4f}")
            print(f"   ‚Ä¢ Median: {sorted(scores)[len(scores)//2]:.4f}")
        
        # Save results
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"service_logic_test_{timestamp}.json"
        
        with open(results_file, 'w') as f:
            json.dump({
                "summary": {
                    "total_tests": len(self.test_messages),
                    "correct_predictions": correct_predictions,
                    "accuracy": accuracy,
                    "timestamp": timestamp,
                    "detector_info": self.detector.get_detector_info()
                },
                "results": results,
                "analysis": {
                    "false_positives": len(false_positives),
                    "false_negatives": len(false_negatives),
                    "score_stats": {
                        "min": min(scores) if scores else 0,
                        "max": max(scores) if scores else 0,
                        "avg": sum(scores)/len(scores) if scores else 0
                    }
                }
            }, f, indent=2, default=str)
        
        print(f"\nüíæ Results saved to: {results_file}")
        
        # Final diagnosis
        print("\nüè• DIAGNOSIS:")
        if accuracy < 0.4:
            print("üö® CRITICAL: Service logic is severely broken")
            print("   ‚Ä¢ Check model loading and initialization")
            print("   ‚Ä¢ Verify toxicity score calculation")
            print("   ‚Ä¢ Check threshold application")
        elif accuracy < 0.6:
            print("‚ö†Ô∏è  WARNING: Service logic has significant issues")
            print("   ‚Ä¢ Review score calculation logic")
            print("   ‚Ä¢ Check threshold settings")
            print("   ‚Ä¢ Verify model output interpretation")
        elif accuracy < 0.8:
            print("‚ö° MODERATE: Service logic needs tuning")
            print("   ‚Ä¢ Consider adjusting thresholds")
            print("   ‚Ä¢ Review edge cases")
        else:
            print("‚úÖ GOOD: Service logic is working well")
            print("   ‚Ä¢ Integration issues may be elsewhere")
            print("   ‚Ä¢ API layer or request handling might be the issue")
        
        return results

def main():
    """Main execution"""
    print("üöÄ Starting Service Logic Test...")
    
    try:
        tester = ServiceLogicTester()
        results = tester.test_service_logic()
        
        print("\n‚úÖ Service logic test completed!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
